{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d26b51c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, BatchSampler, Sampler\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from datetime import datetime \n",
    "import math\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d83d9ef",
   "metadata": {},
   "source": [
    "### DATALOADER\n",
    "\n",
    "The official dataloader from pytorch's official site uses the DataSet Class to load data via the __getitem__ method. The documentation uses numpy as well. I tried finding examples for torch file loaders but it doesn't seem to exist. __getitem__ uses indexes to load data from a csv file (in my example it's train_clean.csv and test_clean.csv. The game and face files are linked in the csv and are actually stored in the processed folder. The same method puts all the video files and their labels in a dictionary and returns it. The method __process_labels__ reads all the labels in the csv files and puts it into 3 different numpy array. \n",
    "\n",
    "The returned object of __getitem__ is: \n",
    "sample = {\"face\": face_data, \"game\": game_data, \"valence\": valence_labels, \"arousal labels\": arousal_labels, \"game event labels\": game_event_labels}\n",
    "\n",
    "Since the data has been normalized already there is no need to further transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4888606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))\n",
    "\n",
    "class StreamLolGame(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, npz_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        data = np.load(npz_file, mmap_mode='r')\n",
    "        self.game_data = torch.from_numpy(data[\"data\"][:,:,:,:])\n",
    "        #self.valence_labels = torch.from_numpy(data[\"valence_labels\"])\n",
    "        #self.arousal_labels = torch.from_numpy(data[\"arousal_labels\"])\n",
    "        self.game_event_labels = torch.from_numpy(data[\"game_event_labels\"])\n",
    "        \n",
    "        del data\n",
    "        self.root_dir = root_dir\n",
    "        \n",
    "        ## Because the image data is taken from short videos, images from the same video look almost the same, so data augmentation is a must to improve the model: \n",
    "        self.transform = transform\n",
    "        \n",
    "        self.game_data = self.game_data.permute(0, 3, 1, 2)\n",
    "        \n",
    "        if self.transform:\n",
    "            self.game_data = self.transform(self.game_data)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.game_data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):        \n",
    "        \n",
    "        if isinstance(idx, list) == False:\n",
    "            idx = [idx]\n",
    "\n",
    "        sample = {\"data\": self.game_data[idx, :, :, :], \"game_event_labels\": self.game_event_labels[idx, :]}\n",
    "                  \n",
    "        return sample\n",
    "    \n",
    "    @staticmethod\n",
    "    def __process_labels__(raw_y_data):\n",
    "        \"\"\"Takes raw labels and seperates them into the correct 'heads'\n",
    "\n",
    "        :param raw_y_data: Raw inputd ata\n",
    "        :return: A tuple of arrays, where each array contains all lables for a given output\n",
    "        \"\"\"\n",
    "        valence = np.array([raw_y_data['V_Neg'], raw_y_data['V_Neut'], raw_y_data['V_Pos']])\n",
    "        arousal = np.array([raw_y_data['A_Neut'], raw_y_data['A_Pos']])\n",
    "\n",
    "        laning = raw_y_data['Laning']\n",
    "        shopping = raw_y_data['Shopping']\n",
    "        returning = raw_y_data['Returning']\n",
    "        roaming = raw_y_data['Roaming']\n",
    "        fighting = raw_y_data['Fighting']\n",
    "        pushing = raw_y_data['Pushing']\n",
    "        defending = raw_y_data['Defending']\n",
    "        dead = raw_y_data['Dead']\n",
    "\n",
    "        game_events = np.array([laning, shopping, returning, roaming, fighting, pushing, defending, dead])\n",
    "        return valence, arousal, game_events\n",
    "\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3fda3e",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "for the model I use a typical CNN setup (Convolution - relu - maxpool - convolution - relu - 3 fully connected layers). For both of my convolutional layers I used 16 filters, 3x3 kernels, a stride of 1x1 and 0 padding. The pooling layer was 4x4. The 3 fully connected layers have input sizes 13456, 120 and 32. The output is a multi-class, single label output predicting the game state. The loss function is calculated using cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "893e31a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#out = ((Input width - filter size + 2* padding) / Stride) + 1\n",
    "#dropout = 0 is shortcutted\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, img_shape=128, label_shape=8, channel_numbers=[3,16], number_of_c_layers = 2, number_of_filters=[16,16], \n",
    "                 kernel_sizes=[3,3], strides=[1,1], paddings=[0,0], fclayout = [120, 32], maxpool = [4, 4], device = \"cuda\", BN=False, dropout_rate = 0):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.channel_number = channel_numbers\n",
    "        self.number_of_filters = number_of_filters\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.strides = strides\n",
    "        self.paddings = paddings\n",
    "        self.maxpool = maxpool\n",
    "        self.fclayout = fclayout\n",
    "        self.fcs = []\n",
    "        self.label_shape = label_shape\n",
    "        self.img_shape = img_shape\n",
    "        self.dropout_rate = dropout_rate\n",
    "            \n",
    "        self.net = nn.Sequential()\n",
    "        \n",
    "        for i in range(number_of_c_layers): \n",
    "            if i == number_of_c_layers-1:\n",
    "                self.net.add_module(\"conv2-\" + str(i), nn.Conv2d(channel_numbers[i], number_of_filters[i], kernel_sizes[i], stride=strides[i], padding=paddings[i]))\n",
    "                if BN:\n",
    "                    self.net.add_module(\"BN-2d-\" + str(i), nn.BatchNorm2d(number_of_filters[i]))\n",
    "                self.net.add_module(\"relu-conv-\" + str(i), nn.ReLU())\n",
    "            else:\n",
    "                self.net.add_module(\"conv2-\" + str(i), nn.Conv2d(channel_numbers[i], number_of_filters[i], kernel_sizes[i], stride=strides[i], padding=paddings[i]))\n",
    "                if BN:\n",
    "                    self.net.add_module(\"BN-2d-\" + str(i), nn.BatchNorm2d(number_of_filters[i]))\n",
    "                self.net.add_module(\"relu-conv-\" + str(i), nn.ReLU())\n",
    "                self.net.add_module(\"maxpool-\" + str(i), nn.MaxPool2d(maxpool[i], maxpool[i]))\n",
    "        \n",
    "        self.net.add_module(\"flatten1\", nn.Flatten())\n",
    "        \n",
    "        fc_down = self.net(torch.rand(1, channel_numbers[0], img_shape, img_shape)).size()[1]\n",
    "        print(fc_down)\n",
    "        \n",
    "        self.net.add_module(\"fc-0\", nn.Linear(fc_down, self.fclayout[0]))\n",
    "        if BN:\n",
    "            print(\"BN3: \", self.fclayout[0])\n",
    "            self.net.add_module(\"BN-1d-0\", nn.BatchNorm1d(self.fclayout[0]))\n",
    "        self.net.add_module(\"Drop-0\", nn.Dropout(dropout_rate))\n",
    "        self.net.add_module(\"relu-fc-0\", nn.ReLU())\n",
    "        \n",
    "        for i in range(len(fclayout)):\n",
    "            if i == len(self.fclayout)-1:\n",
    "                self.net.add_module(\"fc-\" + str(i+1), nn.Linear(fclayout[i], self.label_shape))\n",
    "            else:\n",
    "                self.net.add_module(\"fc-\" + str(i+1), nn.Linear(fclayout[i], fclayout[i+1]))\n",
    "                if BN:\n",
    "                    print(\"BN: \" + str(i+1), fclayout[i+1])\n",
    "                    self.net.add_module(\"BN-1d-\" + str(i+1), nn.BatchNorm1d(fclayout[i+1]))\n",
    "                self.net.add_module(\"Drop-\" + str(i+1), nn.Dropout(dropout_rate))\n",
    "                self.net.add_module(\"relu-fc-\" + str(i+1), nn.ReLU())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dd2eef",
   "metadata": {},
   "source": [
    "#### Prepare Params and load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559ef627",
   "metadata": {},
   "source": [
    "##### What Metrics do we want to look at?\n",
    "\n",
    "The metrics for my model to be considered are the accuracy and the total cross entropy loss for all batches per epoch. This total cross entropy loss would tell us more information about the speed at which the model is learning, telling us about the loss over all batches per epoch. During training the accuracy of the validaiton set is much more interesting than the accuracy of the training set, since the validation set is applying the model to \"new\" data. \n",
    "\n",
    "-----------------\n",
    "\n",
    "The model uses xavier weight initialization for every hidden layer. The model wouldn't train at all without proper weight initialization.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9e85ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "#----------------------------------\n",
    "# Generators\n",
    "\n",
    "#transform leads to full memory\n",
    "#transform=transforms.Compose([AddGaussianNoise(0., 0.2)\n",
    "                            #transforms.RandomPerspective(),\n",
    "                            #transforms.RandomAdjustSharpness(0.8),\n",
    "                            #transforms.RandomAutocontrast()\n",
    "                            #])\n",
    "\n",
    "trainset = StreamLolGame(\"data_game.npz\", \"\") #, transform = transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69416607",
   "metadata": {},
   "source": [
    "#### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6c00f9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)\n",
    "#model.add_module(\"fc\", nn.Linear(512, 8))\n",
    "\n",
    "def training(trainingloader, max_epochs, optimizer, model, validationloader=None, run_name=\"\", l1_lambda = 0):\n",
    "    \n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    now = datetime.now()\n",
    "\n",
    "    current_time = now.strftime(\"%H_%M_%S\")\n",
    "\n",
    "    writer = SummaryWriter(\"runs/\" + run_name)\n",
    "    \n",
    "    metrics_tracker = pd.DataFrame(columns=['epoch', 'train_loss', 'train_accuracy','val_loss', 'val_accuracy'])\n",
    "    epochs_list = []\n",
    "    train_loss_list = []\n",
    "    train_accuracy_list = []\n",
    "    val_loss_list = []\n",
    "    val_accuracy_list = []\n",
    "    \n",
    "    start.record()\n",
    "    for epochs in range(max_epochs):\n",
    "        # Training\n",
    "        correct = 0\n",
    "        total_labels = 0\n",
    "        total_loss = 0\n",
    "        for batch_idx, batch in enumerate(trainingloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #batch[\"data\"][: (all items in current batch) , 0 (first item in list) , : all channels, : (all pixels h), : (all pixels w)] \n",
    "            batch_input = batch[\"data\"][:, 0, :, :, :].to(device).float()\n",
    "\n",
    "            batch_game_event = batch[\"game_event_labels\"][:, 0, :].to(device).long()\n",
    "\n",
    "            prediction = model.forward(batch_input)\n",
    "\n",
    "            argmax_predicton = torch.argmax(prediction, dim = 1)\n",
    "            argmax_game_event = torch.argmax(batch_game_event, dim = 1)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                loss = nn.functional.cross_entropy(prediction, argmax_game_event)\n",
    "            \n",
    "            if l1_lambda != 0:\n",
    "                l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "\n",
    "                loss = loss + l1_lambda * l1_norm\n",
    "\n",
    "            #loss gets updated after each batch, so a total loss is better to see if model is improving\n",
    "            total_loss += loss\n",
    "\n",
    "            total_labels += batch_game_event.size(0)\n",
    "            correct += (argmax_predicton == argmax_game_event).sum().item()\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        print(f'Epoch: [{epochs+1}/{max_epochs}], total train (sum of) Loss: {total_loss}, Train Acc: {correct} / {total_labels}')\n",
    "        writer.add_scalar('Loss/train', total_loss, epochs)\n",
    "        writer.add_scalar('Accuracy/train', correct/total_labels, epochs)\n",
    "        epochs_list.append(epochs+1)\n",
    "        train_loss_list.append(total_loss.cpu().detach())\n",
    "        train_accuracy_list.append(correct/total_labels)\n",
    "        \n",
    "        if validationloader:\n",
    "            correct = 0\n",
    "            total_labels = 0\n",
    "            total_loss = 0\n",
    "            model.eval()  # handle drop-out/batch norm layers\n",
    "            loss = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, batch in enumerate(validationloader):\n",
    "\n",
    "                    #batch[\"face\"][: (all items in current batch) , 0 (first item in list) , 1 (first frame), : (all pixels h), : (all pixels w)]\n",
    "                    batch_input = batch[\"data\"][:, 0, :, :, :].to(device).float()\n",
    "\n",
    "                    batch_game_event = batch[\"game_event_labels\"][:, 0, :].to(device).long()\n",
    "                    \n",
    "                    argmax_game_event = torch.argmax(batch_game_event, dim = 1)\n",
    "                    \n",
    "                    prediction = model(batch_input)  # only forward pass - NO gradients!!\n",
    "                    loss += nn.functional.cross_entropy(prediction, argmax_game_event)\n",
    "                    \n",
    "                    argmax_predicton = torch.argmax(prediction, dim = 1)\n",
    "                    argmax_game_event = torch.argmax(batch_game_event, dim = 1)\n",
    "                    \n",
    "                    total_labels += batch_game_event.size(0)\n",
    "                    correct += (argmax_predicton == argmax_game_event).sum().item()\n",
    "                    \n",
    "                # total loss - divide by number of batches\n",
    "                val_loss = loss / len(validationloader)\n",
    "                \n",
    "                print(f'Epoch: [{epochs+1}/{max_epochs}], total validation (sum of) Loss: {val_loss}, Validation Acc: {correct} / {total_labels}')\n",
    "                writer.add_scalar('Loss/validation', val_loss, epochs)\n",
    "                writer.add_scalar('Accuracy/validation', correct/total_labels, epochs)\n",
    "                val_loss_list.append(val_loss.cpu().detach())\n",
    "                val_accuracy_list.append(correct/total_labels)\n",
    "        \n",
    "    metrics_tracker = [{'epoch': epochs_list[i], 'train_loss': train_loss_list[i].item(), 'train_accuracy': train_accuracy_list[i], 'val_loss': val_loss_list[i].item(), 'val_accuracy': val_accuracy_list[i]} for i in range(max_epochs)]\n",
    "    \n",
    "    end.record()\n",
    "    # Waits for everything to finish running\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    print(start.elapsed_time(end)/1000/60, \"min\")\n",
    "    torch.cuda.empty_cache()\n",
    "    return pd.DataFrame(metrics_tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82f8ffa2-89b0-4dc8-8453-e14e03ddb2e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Aufgabe 3\n",
    "# SGD ohne Reg ohne BN\n",
    "# 3 verschiedene LR und Batch Grössen\n",
    "# Verschiedene Anzahl Filter, Kernel Grössen Stride Padding\n",
    "# Plot alles zusammen\n",
    "\n",
    "## Aufgabe 4\n",
    "# für alle in Aufgabe 3 gemachten Plots, L1/L2 Weight Penalty und 3 verschiedene Dropoutraten\n",
    "\n",
    "## Aufgabe 5\n",
    "# BN ohne REG mit SGD\n",
    "\n",
    "## Aufgabe 6\n",
    "# Adam, ohne BN ohne/mit REG\n",
    "\n",
    "metrics_path = \"trained_history/\"\n",
    "saved_models_path = \"trained_models/\"\n",
    "\n",
    "params_b_64 = {'batch_size': 64, \n",
    "          'shuffle': True, \n",
    "          'pin_memory': True}\n",
    "\n",
    "params_b_128 = {'batch_size': 128, \n",
    "          'shuffle': True, \n",
    "          'pin_memory': True}\n",
    "\n",
    "params_b_256 = {'batch_size': 256, \n",
    "          'shuffle': True, \n",
    "          'pin_memory': True}\n",
    "\n",
    "#add to this for pipeline\n",
    "params = [params_b_64]\n",
    "\n",
    "#add to this for pipeline\n",
    "lrs = [0.01]\n",
    "\n",
    "dropouts = [0.1, 0.25, 0.5]\n",
    "\n",
    "val_params = {'batch_size': 512,\n",
    "                'shuffle': True, \n",
    "              'pin_memory': True}\n",
    "\n",
    "max_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4237a090-e8e8-46fc-b81d-f311f553ca63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26912\n",
      "CNN(\n",
      "  (net): Sequential(\n",
      "    (conv2-0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (relu-conv-0): ReLU()\n",
      "    (maxpool-0): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2-1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (relu-conv-1): ReLU()\n",
      "    (flatten1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc-0): Linear(in_features=26912, out_features=120, bias=True)\n",
      "    (Drop-0): Dropout(p=0, inplace=False)\n",
      "    (relu-fc-0): ReLU()\n",
      "    (fc-1): Linear(in_features=120, out_features=32, bias=True)\n",
      "    (Drop-1): Dropout(p=0, inplace=False)\n",
      "    (relu-fc-1): ReLU()\n",
      "    (fc-2): Linear(in_features=32, out_features=8, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch: [1/200], total train (sum of) Loss: 713.037109375, Train Acc: 4946 / 22893\n",
      "Epoch: [1/200], total validation (sum of) Loss: 1.978102684020996, Validation Acc: 976 / 4039\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17168/2461037267.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mtrainingloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mrun_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"lr-\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_batch_size-\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'batch_size'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"-kernel100\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidationloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrun_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaved_models_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17168/3574667741.py\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(trainingloader, max_epochs, optimizer, model, validationloader, run_name, l1_lambda)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m             \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    336\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"found_inf_per_device\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"No inf checks were recorded for this optimizer.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m         \u001b[0mretval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"stage\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[1;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_maybe_opt_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m         \u001b[0mretval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"found_inf_per_device\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    285\u001b[0m             \u001b[0mretval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_maybe_opt_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m         \u001b[0mretval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"found_inf_per_device\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    285\u001b[0m             \u001b[0mretval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_set, val_set = torch.utils.data.random_split(trainset, [math.ceil(0.85*len(trainset)), math.floor(0.15*len(trainset))])\n",
    "\n",
    "validationloader = DataLoader(val_set, **val_params)\n",
    "\n",
    "for lr in lrs:\n",
    "    for param in params:\n",
    "        #for dropout in dropouts:\n",
    "        #print(str(dropout))\n",
    "\n",
    "        model = CNN(number_of_c_layers = 2, number_of_filters=[16,32], channel_numbers=[3,16], maxpool=[4,4], kernel_sizes=[3,3], strides=[1, 1], paddings=[0,0], fclayout=[120,32])#, dropout_rate = dropout)\n",
    "\n",
    "        model.cuda()\n",
    "        model.apply(weights_init)\n",
    "\n",
    "        print(model)\n",
    "        \n",
    "        trainingloader = DataLoader(train_set, **param)\n",
    "        run_name = \"lr-\" + str(lr) + \"_batch_size-\" + str(param['batch_size']) + \"-kernel100\"\n",
    "        results = training(trainingloader, max_epochs, torch.optim.Adam(model.parameters(),lr=0.0001), model, validationloader, run_name=run_name)\n",
    "        results.to_csv(metrics_path + run_name + \".csv\", index=False)\n",
    "        torch.save(model.state_dict(), saved_models_path + run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d2d43d-e6c2-4a18-b2cf-9ffbcf72772f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(saved_models_path + \"lr-0.1_batch_size-64-kernel-5\")\n",
    "print(checkpoint)\n",
    "model.load_state_dict(checkpoint)\n",
    "print(model)\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "trainingloader = DataLoader(train_set, **params_b_64)\n",
    "run_name = \"lr-0.1_batch_size-64-kernel-5\"\n",
    "results = training(trainingloader, max_epochs, torch.optim.SGD(model.parameters(),lr=0.01), model, validationloader, run_name=run_name)\n",
    "results.to_csv(metrics_path + run_name + \".csv\", index=False)\n",
    "torch.save(model.state_dict(), saved_models_path + run_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
